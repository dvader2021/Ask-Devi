{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "588c6b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load EDA\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.metrics.pairwise import cosine_similarity,linear_kernel\n",
    "\n",
    "# other packages\n",
    "import nltk\n",
    "import string\n",
    "import ast\n",
    "import re\n",
    "import unidecode\n",
    "import unicodedata\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "\n",
    "import pickle \n",
    "import unidecode, ast\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3574927",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/sangames/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/sangames/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sangames/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/sangames/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# that's all the libaries that we need .........."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05c4283c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to all files\n",
    "DATA_PATH = \"./data/updated_saree_data_v2.csv\"\n",
    "CLEAN_PATH = \"./data/data_parsed_new.csv\"\n",
    "TFIDF_ENCODING_PATH = \"./model/data_tfidf_encodings.pkl\"\n",
    "TFIDF_MODEL_PATH = \"./model/data_tfidf.pkl\"\n",
    "\n",
    "#Initialising stopwords for english\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Load Our Dataset\n",
    "def loadData(fileName): \n",
    "    df = pd.read_csv(fileName)\n",
    "    return df \n",
    "\n",
    "#save the file\n",
    "def saveData(df, fileName): \n",
    "    df.to_csv(fileName, index=False)  \n",
    "    \n",
    "\n",
    "def parseDataFile():\n",
    "    \n",
    "    # parses the data into words\n",
    "    rec_df = loadData(DATA_PATH)\n",
    "\n",
    "    # change the way the sentence is arranged in the data\n",
    "    #rec_df['ingredients'] = rec_df['ingredients'].map(str) + ',' + rec_df['recipe_name'].map(str)\n",
    "    rec_df['desc_temp'] = rec_df['desc'].str.split()\n",
    "\n",
    "    rec_df['keywords'] = rec_df['desc_temp'].apply(lambda x: ingredient_parser(x))\n",
    "\n",
    "    df = rec_df[['id', 'title','desc', 'keywords', 'img_url', 'found']]\n",
    "    df = rec_df.dropna()\n",
    "\n",
    "    saveData(df,CLEAN_PATH)\n",
    "\n",
    "def createModel():\n",
    "   \n",
    "    # load in parsed recipe dataset     \n",
    "    df_rec = loadData(CLEAN_PATH)\n",
    "    df_rec['keywords'] = df_rec.desc.values.astype('U')\n",
    "\n",
    "    # TF-IDF feature extractor \n",
    "    tfidf = TfidfVectorizer()\n",
    "    tfidf.fit(df_rec['keywords'])\n",
    "    tfidf_recipe = ve???? check code\n",
    "    \n",
    "    # save the tfidf model and encodings \n",
    "    with open(TFIDF_MODEL_PATH, \"wb\") as f:\n",
    "        pickle.dump(tfidf, f)\n",
    "\n",
    "    with open(TFIDF_ENCODING_PATH, \"wb\") as f:\n",
    "        pickle.dump(tfidf_recipe, f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "598d82a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def title_parser(title):\n",
    "    title = unidecode.unidecode(title)\n",
    "    return title \n",
    "\n",
    "def ingredient_parser(ingreds):\n",
    "    \n",
    "    #showStatus(\"ingredient parser\")\n",
    "    user_selected_words_to_remove = ['(',')','.','\\'','saree','matching', 'lehenga','women', 'woman','shubh','self','fresh','a', 'and',  'or', 'large', 'extra', 'free', 'small', 'from', 'higher', 'for', 'the', 'plain', 'plus' ]\n",
    "    # The ingredient list is now a string so we need to turn it back into a list. We use ast.literal_eval\n",
    "    if isinstance(ingreds, list):\n",
    "        ingredients = ingreds\n",
    "    else:\n",
    "        ingredients = ast.literal_eval(ingreds)\n",
    "\n",
    "    # We first get rid of all the punctuation. We make use of str.maketrans. It takes three input \n",
    "    # arguments 'x', 'y', 'z'. 'x' and 'y' must be equal-length strings and characters in 'x'\n",
    "    # are replaced by characters in 'y'. 'z' is a string (string.punctuation here) where each character\n",
    "    #  in the string is mapped to None. \n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    ingred_list = []\n",
    "    for i in ingredients:\n",
    "        i.translate(translator)\n",
    "        # We split up with hyphens as well as spaces\n",
    "        items = re.split(' |-', i)\n",
    "        # Get rid of words containing non alphabet letters\n",
    "        items = [word for word in items if word.isalpha()]\n",
    "        # Turn everything to lowercase\n",
    "        items = [word.lower() for word in items]      \n",
    "        # remove stop words\n",
    "        items = [word for word in items if word not in stop_words]        \n",
    "        # remove accents\n",
    "        items = [unidecode.unidecode(word) for word in items] #''.join((c for c in unicodedata.normalize('NFD', items) if unicodedata.category(c) != 'Mn'))\n",
    "        items = [unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore') for word in items]        \n",
    "        # Lemmatize words so we can compare words to measuring words\n",
    "        items = [lemmatizer.lemmatize(word) for word in items]\n",
    "        # Get rid of other user selected words\n",
    "        items = [word for word in items if word not in user_selected_words_to_remove]\n",
    "        # remove all square brackets\n",
    "        items = [remove_between_square_brackets(word) for word in items]\n",
    "        # remove all special characters\n",
    "        items = [remove_special_characters(word) for word in items]\n",
    "        if items:\n",
    "            ingred_list.append(' '.join(items)) \n",
    "    ingred_list = \" \".join(ingred_list)\n",
    "    return ingred_list\n",
    "\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "def remove_special_characters(text, remove_digits=True):\n",
    "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2321f6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "parseDataFile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d07932b",
   "metadata": {},
   "outputs": [],
   "source": [
    "createModel()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bb7d25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python396jvsc74a57bd095ec9ec1504d83f612128e0fb229072f90bbb4cb09d9d5d93b5dd26e0ca2cfd1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
